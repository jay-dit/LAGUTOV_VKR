{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78614588",
   "metadata": {},
   "source": [
    "# Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a5fab7d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils import Disassembled_DDL, diff_DDLs, main_alerting_function, alter_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52921414",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "pd.DataFrame.iteritems = pd.DataFrame.items\n",
    "\n",
    "import time\n",
    "\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "import smtplib\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8fe071",
   "metadata": {},
   "source": [
    "# Демонстрация основных модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5a9e569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .config(\"spark.driver.memory\", \"1g\")\\\n",
    "    .config(\"spark.executor.memory\", '2g')\\\n",
    "    .config(\"spark.executor.cores\", '2')\\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\")\\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52ec6298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jovyan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7efce474a130>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "394dc444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "DROP TABLE IF EXISTS new.test\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25bad03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE new.test (\n",
    "  col2 DECIMAL(2,0),\n",
    "  col3 DECIMAL(1,0))\n",
    "USING orc\n",
    "PARTITIONED BY (col2)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c0c8ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl1 = spark.sql(\"SHOW CREATE TABLE new.test\").toPandas().iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3cab231",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl2 = spark.sql(\"SHOW CREATE TABLE new.stat\").toPandas().iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5d84245",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_ddl1 = Disassembled_DDL(ddl1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "628fe520",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_ddl2 = Disassembled_DDL(ddl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22e6e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "printed = diff_DDLs(D_ddl1, D_ddl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38b4d016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following fields have been deleted from the table: col3, col2\n",
      "The following fields have been added to the table\n",
      "    num_partitions bigint\n",
      "    flag bigint\n",
      "    time bigint\n",
      "    min_f bigint\n",
      "    cpu string\n",
      "    min_v double\n",
      "    max_f bigint\n",
      "    volume bigint\n",
      "    mem string\n",
      "    max_v double\n",
      "\n",
      "The following partitions fields have been deleted from the table: col2\n",
      "\n",
      "=====================================\n",
      "table_db_name\n",
      "=====================================\n",
      "new.test\n",
      "-------------------------------------\n",
      "changed_on\n",
      "-------------------------------------\n",
      "new.stat\n",
      "=====================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in printed.items():\n",
    "    if value:\n",
    "        print(value[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2fe1579",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_alerting_function(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e95ae7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "start testing DDLs\n",
      "Success create table from file\n",
      "Success drop table from file\n",
      "Success create table new_DDL\n",
      "Success drop table new_DDL\n",
      "===============================================\n",
      "start proccess\n",
      "start create backup table\n",
      "start upload data to backup table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succes cnt_old == cnt_backup\n",
      "Finish upload data to backup table\n",
      "===============================================\n",
      "Start drop and recreate main table\n",
      "Success recreate\n",
      "start upload data to main table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success. Finish work\n"
     ]
    }
   ],
   "source": [
    "table_name = 'new.test'\n",
    "\n",
    "\n",
    "new_DDL = \"\"\"\n",
    "CREATE TABLE new.test (\n",
    "  col1 DECIMAL(1,0),\n",
    "  col2 DECIMAL(1,0))\n",
    "USING orc\n",
    "PARTITIONED BY (col2)\n",
    "\"\"\"\n",
    "\n",
    "select_statement = \"\"\"\n",
    "SELECT NULL as col1,\n",
    "       col2\n",
    "FROM new.test__backup\n",
    "\"\"\"\n",
    "\n",
    "alter_table(spark, table_name, new_DDL, select_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82381a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_alerting_function(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46222879",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b1ed414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table(spark, name, num_lines):\n",
    "    \n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {name}\")\n",
    "    \n",
    "    shape = (num_lines, 5)\n",
    "\n",
    "    data = np.random.randint(0, 20, size=shape)\n",
    "    df = pd.DataFrame(data, columns = [str(i) for i in range(shape[1])])\n",
    "\n",
    "    df_spark = spark.createDataFrame(df)\n",
    "\n",
    "    df_spark.coalesce(1).write.format('ORC').saveAsTable(name, partitionBy = [str(i) for i in range(shape[1])][-1])\n",
    "\n",
    "    shape = (num_lines, 5)\n",
    "\n",
    "    data = np.random.randint(0, 7, size=shape)\n",
    "    df = pd.DataFrame(data, columns = [str(i) for i in range(shape[1])])\n",
    "\n",
    "    df_spark = spark.createDataFrame(df)\n",
    "\n",
    "    df_spark.coalesce(1).write.format('ORC').insertInto(name)\n",
    "\n",
    "    shape = (num_lines, 5)\n",
    "\n",
    "    data = np.random.randint(15, 20, size=shape)\n",
    "    df = pd.DataFrame(data, columns = [str(i) for i in range(shape[1])])\n",
    "\n",
    "    df_spark = spark.createDataFrame(df)\n",
    "\n",
    "    df_spark.coalesce(1).write.format('ORC').insertInto(name)\n",
    "    \n",
    "    shape = (num_lines, 5)\n",
    "\n",
    "    data = np.random.randint(2, 16, size=shape)\n",
    "    df = pd.DataFrame(data, columns = [str(i) for i in range(shape[1])])\n",
    "\n",
    "    df_spark = spark.createDataFrame(df)\n",
    "\n",
    "    df_spark.coalesce(1).write.format('ORC').insertInto(name)\n",
    "    \n",
    "    shape = (num_lines, 5)\n",
    "\n",
    "    data = np.random.randint(8, 12, size=shape)\n",
    "    df = pd.DataFrame(data, columns = [str(i) for i in range(shape[1])])\n",
    "\n",
    "    df_spark = spark.createDataFrame(df)\n",
    "\n",
    "    df_spark.coalesce(1).write.format('ORC').insertInto(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c0f0a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic(table_name):\n",
    "    schema, name = table_name.split('.')\n",
    "    path = f'/user/hive/warehouse/{schema}.db/{name}/'\n",
    "    \n",
    "    command = subprocess.check_output([f'hdfs dfs -du -s {path}'], shell=True)\n",
    "    command = command.decode()\n",
    "    \n",
    "    mem_without_replic = int(command.split('  ')[0])\n",
    "    \n",
    "    command = subprocess.check_output([f'hdfs dfs -find {path} -name \"*.orc\"'], shell=True)\n",
    "    command = command.decode()\n",
    "    \n",
    "    all_files = command.split('\\n')[:-1]\n",
    "    \n",
    "    all_dirs = set(map(lambda x: '/'.join(x.split('/')[:-1]), all_files))\n",
    "    \n",
    "    cnt_files_in_directory = {}\n",
    "    \n",
    "    for el in all_dirs:\n",
    "        cnt_files_in_directory[el] = 0\n",
    "    \n",
    "    for el in all_files:\n",
    "        key = '/'.join(el.split('/')[:-1])\n",
    "        cnt_files_in_directory[key] += 1\n",
    "    \n",
    "    min_files = min(cnt_files_in_directory.values())\n",
    "    max_files = max(cnt_files_in_directory.values())\n",
    "    \n",
    "    min_value = min_files * (int(mem_without_replic)/len(all_files))\n",
    "    max_value = max_files * (int(mem_without_replic)/len(all_files))\n",
    "                       \n",
    "    all_parts = len(all_dirs)\n",
    "    \n",
    "    return (mem_without_replic, all_parts, min_files, max_files, min_value, max_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a34db947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[183406, 20, 2, 3, 7336.24, 11004.36, '2', '2g', 1, 33]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_table(name, num_lines, core, memory):\n",
    "    t1 = time.time()\n",
    "    \n",
    "    spark = SparkSession.builder\\\n",
    "        .config(\"spark.driver.memory\", \"1g\")\\\n",
    "        .config(\"spark.executor.memory\", str(memory))\\\n",
    "        .config(\"spark.executor.cores\", str(core))\\\n",
    "        .config(\"hive.exec.dynamic.partition\", \"true\")\\\n",
    "        .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    generate_table(spark, name, num_lines)\n",
    "    \n",
    "    st = list(statistic(name))\n",
    "    \n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {name}_tmp\")\n",
    "    \n",
    "    spark.sql(f\"CREATE TABLE {name}_tmp LIKE {name}\")\n",
    "    \n",
    "    try:\n",
    "        spark.sql(f\"SELECT * FROM {name}\").write.format('ORC').insertInto(f'{name}_tmp')\n",
    "    except:\n",
    "        spark.sql(f\"DROP TABLE {name}_tmp\")\n",
    "        spark.sql(f\"DROP TABLE {name}\")\n",
    "        spark.stop()\n",
    "        \n",
    "        st.append(core)\n",
    "        st.append(memory)\n",
    "        st.append(0)\n",
    "        t2 = time.time()\n",
    "        st.append(round(t2 - t1))\n",
    "        return st\n",
    "    \n",
    "    spark.sql(f\"DROP TABLE {name}_tmp\")\n",
    "    spark.sql(f\"DROP TABLE {name}\")\n",
    "    spark.stop()\n",
    "\n",
    "    st.append(core)\n",
    "    st.append(memory)\n",
    "    st.append(1)\n",
    "    t2 = time.time()\n",
    "    st.append(round(t2 - t1))\n",
    "    return st\n",
    "\n",
    "test_table('new.test2', 10_000, '2', '2g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7116f222",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in range(1, 6):\n",
    "    for j in range(1, 8):\n",
    "        for l in range(10_000, 500_000, 30_000):\n",
    "            a.append(test_table('new.test2', l, f'i', f'{j}g'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b93d9990",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(a, columns = ['volume', 'num_partitions', 'min_f', 'max_f', 'min_v', 'max_v', 'cpu', 'mem', 'flag', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "817b2973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .config(\"spark.driver.memory\", \"1g\")\\\n",
    "        .config(\"spark.executor.memory\", '4g')\\\n",
    "        .config(\"spark.executor.cores\", 4)\\\n",
    "        .config(\"hive.exec.dynamic.partition\", \"true\")\\\n",
    "        .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a37e4237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_spark = spark.createDataFrame(df)\n",
    "\n",
    "df_spark.coalesce(1).write.format('ORC').saveAsTable('new.stat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c0f83de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>num_partitions</th>\n",
       "      <th>min_f</th>\n",
       "      <th>max_f</th>\n",
       "      <th>min_v</th>\n",
       "      <th>max_v</th>\n",
       "      <th>cpu</th>\n",
       "      <th>mem</th>\n",
       "      <th>flag</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>183709</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7353.64</td>\n",
       "      <td>11030.46</td>\n",
       "      <td>2</td>\n",
       "      <td>2g</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1525973</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>61038.92</td>\n",
       "      <td>91558.38</td>\n",
       "      <td>3</td>\n",
       "      <td>4g</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2998802</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>119952.08</td>\n",
       "      <td>179928.10</td>\n",
       "      <td>4</td>\n",
       "      <td>7g</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    volume  num_partitions  min_f  max_f      min_v      max_v cpu mem  flag  \\\n",
       "0   183709              10      1      3    7353.64   11030.46   2  2g     1   \n",
       "1  1525973              15      2      7   61038.92   91558.38   3  4g     1   \n",
       "2  2998802              20      3      3  119952.08  179928.10   4  7g     1   \n",
       "\n",
       "   time  \n",
       "0    16  \n",
       "1    35  \n",
       "2    60  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * \n",
    "FROM new.stat\n",
    "LIMIT 3\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0475a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "SELECT volume,\n",
    "       num_partitions,\n",
    "       min_f,\n",
    "       max_f,\n",
    "       min_v,\n",
    "       max_v,\n",
    "       cpu,\n",
    "       mem\n",
    "FROM(\n",
    "    SELECT *,\n",
    "        ROW_NUMBER() over(PARTITION BY volume order by time) as d_id\n",
    "    FROM new.stat\n",
    "    )\n",
    "WHERE d_id <= 3\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8593ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6be68296",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mem'] = df['mem'].map(lambda x: int(x.replace('g','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "17eb7f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.select_dtypes(exclude=['float']).columns\n",
    "df[cols] = df[cols].apply(pd.to_numeric, downcast='float', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fdb589e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['num_partitions', 'volume', 'min_f', 'max_f', 'min_v', 'max_v']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "80a50366",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['cpu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5c944656",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d0ac64aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_cpu = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b761693e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8443069529099603"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_cpu.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "308af81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.34240582e-04,  7.67071198e-05,  1.20037864e-04,  1.02558832e-02,\n",
       "        1.47215567e-03, -2.24814088e-03])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_cpu.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9d69a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mem = df['mem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8b76c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trаin_mem, X_test, y_trаin_mem, y_test = train_test_split(X, y_mem, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "23efdec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_mem = LinearRegression().fit(X_train_mem, y_train_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "56c9ca68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.946123654799294"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_mem.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "856acecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00468423, -0.00030779, -0.00105881, -0.09042189, -0.01664148,\n",
       "        0.01625429])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_mem.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3af71afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def table_parametrs(table_name):\n",
    "    stat = np.array(statistic(table_name))\n",
    "    \n",
    "    b1_mem = np.array([-0.00468423, -0.00030779, -0.00105881, -0.09042189, -0.01664148, 0.01625429])\n",
    "    b0_mem = 1.946123654799294\n",
    "    \n",
    "    b1_cpu = np.array([5.34240582e-04,  7.67071198e-05,  1.20037864e-04,  1.02558832e-02, 1.47215567e-03, -2.24814088e-03])\n",
    "    b0_cpu = 1.8443069529099603\n",
    "    \n",
    "    pred_mem = sum(stat*b1_mem) + b0_mem\n",
    "    pred_cpu = sum(stat*b1_cpu) + b0_cpu\n",
    "    \n",
    "    if pred_mem > 7:\n",
    "        pred_mem = 7\n",
    "    if pred_mem < 1:\n",
    "        pred_mem = 1\n",
    "        \n",
    "    if pred_cpu > 5:\n",
    "        pred_cpu = 5\n",
    "    if pred_cpu < 1:\n",
    "        pred_cpu = 1\n",
    "    \n",
    "    return (int(pred_mem), int(pred_cpu))\n",
    "table_parametrs('new.stat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
